"use strict";(self.webpackChunkfresher_training_doc=self.webpackChunkfresher_training_doc||[]).push([[757],{388:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var t=n(4848),a=n(8453);const i={},o="Image Processing",s={id:"WebGL/webgl-image-processing",title:"Image Processing",description:"To draw images in WebGL we need to use textures. Similarly to the way WebGL expects clip space coordinates when rendering instead of pixels, WebGL expects texture coordinates when reading a texture. Texture coordinates go from 0.0 to 1.0 no matter the dimensions of the texture.",source:"@site/docs/WebGL/webgl-image-processing.md",sourceDirName:"WebGL",slug:"/WebGL/webgl-image-processing",permalink:"/fresher-training-doc/docs/WebGL/webgl-image-processing",draft:!1,unlisted:!1,editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/WebGL/webgl-image-processing.md",tags:[],version:"current",frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"How it works",permalink:"/fresher-training-doc/docs/WebGL/webgl-how-it-works"},next:{title:"Introduction",permalink:"/fresher-training-doc/docs/PixiJS/introduction"}},l={},c=[{value:"Image effects",id:"image-effects",level:3}];function g(e){const r={a:"a",code:"code",h1:"h1",h3:"h3",header:"header",img:"img",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(r.header,{children:(0,t.jsx)(r.h1,{id:"image-processing",children:"Image Processing"})}),"\n",(0,t.jsx)(r.p,{children:"To draw images in WebGL we need to use textures. Similarly to the way WebGL expects clip space coordinates when rendering instead of pixels, WebGL expects texture coordinates when reading a texture. Texture coordinates go from 0.0 to 1.0 no matter the dimensions of the texture."}),"\n",(0,t.jsx)(r.p,{children:"Since we are only drawing a single rectangle (well, 2 triangles) we need to tell WebGL which place in the texture each point in the rectangle corresponds to. We'll pass this information from the vertex shader to the fragment shader using a special kind of variable called a 'varying'. It's called a varying because it varies. WebGL will interpolate the values we provide in the vertex shader as it draws each pixel using the fragment shader."}),"\n",(0,t.jsxs)(r.p,{children:["Using the ",(0,t.jsx)(r.a,{href:"/fresher-training-doc/docs/WebGL/webgl-funamentals",children:"vertex shader from the end of the previous lesson"})," we need to add an attribute to pass in texture coordinates and then pass those on to the fragment shader."]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-javascript",children:"attribute vec2 a_texCoord;\r\n...\r\nvarying vec2 v_texCoord;\r\n \r\nvoid main() {\r\n   ...\r\n   // pass the texCoord to the fragment shader\r\n   // The GPU will interpolate this value between points\r\n   v_texCoord = a_texCoord;\r\n}\n"})}),"\n",(0,t.jsx)(r.p,{children:"Then we supply a fragment shader to look up colors from the texture."}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-javascript",children:"precision mediump float;\r\n \r\n// our texture\r\nuniform sampler2D u_image;\r\n \r\n// the texCoords passed in from the vertex shader.\r\nvarying vec2 v_texCoord;\r\n \r\nvoid main() {\r\n   // Look up a color from the texture.\r\n   gl_FragColor = texture2D(u_image, v_texCoord);\r\n}\n"})}),"\n",(0,t.jsxs)(r.p,{children:["Finally, we need to load ",(0,t.jsx)(r.a,{target:"_blank","data-noBrokenLinkCheck":!0,href:n(2234).A+"",children:"Mr Survivor image"}),", create a texture and copy the image into the texture. Because we are in a browser images load asynchronously so we have to re-arrange our code a little to wait for the texture to load. Once it loads we'll draw it."]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-javascript",children:'var image = new Image();\r\nimage.src = "./mr-survivor.jpg";\r\nimage.onload = function() {\r\n    render(image);\r\n}\r\n \r\nfunction render(image) {\r\n    ...\r\n    var texcoordLocation = gl.getAttribLocation(program, "a_texCoord");\r\n    \r\n    // Create a buffer to put three 2d clip space points in\r\n    var positionBuffer = gl.createBuffer();\r\n    gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);\r\n\r\n    setRectangle(gl, 0, 0, image.width, image.height);\r\n\r\n\r\n    // Create a texture.\r\n    var texture = gl.createTexture();\r\n    gl.bindTexture(gl.TEXTURE_2D, texture);\r\n\r\n    // Set the parameters so we can render any size image.\r\n    gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);\r\n    gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);\r\n    gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST);\r\n    gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.NEAREST);\r\n\r\n    // Upload the image into the texture.\r\n    gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, image);\r\n\r\n    // Create view port\r\n    gl.viewport(0, 0, gl.canvas.width, gl.canvas.height);\r\n\r\n    // Clear the canvas\r\n    gl.clearColor(0, 0, 0, 0);\r\n    gl.clear(gl.COLOR_BUFFER_BIT);\r\n\r\n    // Tell it to use our program (pair of shaders)\r\n    gl.useProgram(program);\r\n\r\n    // Set the resolution\r\n    gl.uniform2f(resolutionUniformLocation, gl.canvas.width, gl.canvas.height);\r\n\r\n\r\n    // Turn on position attribute\r\n    gl.enableVertexAttribArray(positionAttributeLocation);\r\n\r\n    // Bind the position buffer.\r\n    gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);\r\n\r\n    // Tell the attribute how to get data out of positionBuffer (ARRAY_BUFFER)\r\n    var size = 2;          // 2 components per iteration\r\n    var type = gl.FLOAT;   // the data is 32bit floats\r\n    var normalize = false; // don\'t normalize the data\r\n    var stride = 0;        // 0 = move forward size * sizeof(type) each iteration to get the next position\r\n    var offset = 0;        // start at the beginning of the buffer\r\n    gl.vertexAttribPointer(positionAttributeLocation, size, type, normalize, stride, offset)\r\n\r\n    // Turn on the texcoord attribute\r\n    gl.enableVertexAttribArray(texcoordLocation);\r\n\r\n    // Bind the texcoord buffer.\r\n    gl.bindBuffer(gl.ARRAY_BUFFER, texcoordBuffer);\r\n\r\n    // Tell the texcoord attribute how to get data out of texcoordBuffer (ARRAY_BUFFER)\r\n    var size = 2;          // 2 components per iteration\r\n    var type = gl.FLOAT;   // the data is 32bit floats\r\n    var normalize = false; // don\'t normalize the data\r\n    var stride = 0;        // 0 = move forward size * sizeof(type) each iteration to get the next position\r\n    var offset = 0;        // start at the beginning of the buffer\r\n    gl.vertexAttribPointer(texcoordLocation, size, type, normalize, stride, offset);\r\n    \r\n    // Draw the rectangle.\r\n    var primitiveType = gl.TRIANGLES;\r\n    var offset = 0;\r\n    var count = 6;\r\n    gl.drawArrays(primitiveType, offset, count);\r\n}\n'})}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.img,{src:n(6256).A+"",width:"399",height:"299"})}),"\n",(0,t.jsx)(r.p,{children:"Let just swapping red and blue and see what happens."}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-javascript",children:"gl_FragColor = texture2D(u_image, v_texCoord).bgra;\n"})}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.img,{src:n(1234).A+"",width:"398",height:"299"})}),"\n",(0,t.jsxs)(r.p,{children:["What if we want to do image processing that actually looks at other pixels? Since WebGL references textures in texture coordinates which go from 0.0 to 1.0 then we can calculate how much to move for 1 pixel with the simple math ",(0,t.jsx)(r.code,{children:"onePixel = 1.0 / textureSize"}),"."]}),"\n",(0,t.jsx)(r.p,{children:"Here's a fragment shader that averages the left and right pixels of each pixel in the texture"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-javascript",children:"precision mediump float;\r\n \r\n// our texture\r\nuniform sampler2D u_image;\r\nuniform vec2 u_textureSize;\r\n \r\n// the texCoords passed in from the vertex shader.\r\nvarying vec2 v_texCoord;\r\n \r\nvoid main() {\r\n   // compute 1 pixel in texture coordinates.\r\n   vec2 onePixel = vec2(1.0, 1.0) / u_textureSize;\r\n \r\n   // average the left, middle, and right pixels.\r\n   gl_FragColor = (\r\n       texture2D(u_image, v_texCoord) +\r\n       texture2D(u_image, v_texCoord + vec2(onePixel.x, 0.0)) +\r\n       texture2D(u_image, v_texCoord + vec2(-onePixel.x, 0.0))) / 3.0;\r\n}\n"})}),"\n",(0,t.jsx)(r.p,{children:"We then need to pass in the size of the texture from JavaScript."}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-javascript",children:'...\r\n\r\nvar textureSizeLocation = gl.getUniformLocation(program, "u_textureSize");\r\n\r\n...\r\n\r\n// set the size of the image\r\ngl.uniform2f(textureSizeLocation, image.width, image.height);\r\n\r\n...\r\n\n'})}),"\n",(0,t.jsx)(r.p,{children:"Compare to the un-blurred image above."}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.img,{src:n(422).A+"",width:"1195",height:"898"})}),"\n",(0,t.jsx)(r.h3,{id:"image-effects",children:"Image effects"}),"\n",(0,t.jsxs)(r.p,{children:["Now that we know how to reference other pixels let's use a ",(0,t.jsx)(r.a,{href:"https://en.wikipedia.org/wiki/Kernel_(image_processing)",children:"convolution kernel"})," to do a bunch of common image processing. In this case we'll use a 3x3 kernel. A convolution kernel is just a 3x3 matrix where each entry in the matrix represents how much to multiply the 8 pixels around the pixel we are rendering. We then divide the result by the weight of the kernel (the sum of all values in the kernel) or 1.0, whichever is greater. ",(0,t.jsx)(r.a,{href:"https://docs.gimp.org/2.10/en/gimp-filter-convolution-matrix.html",children:"Here's a pretty good article on it"}),"."]}),"\n",(0,t.jsx)(r.p,{children:"In our case we're going to do that work in the shader so here's the new fragment shader."}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-javascript",children:"precision mediump float;\r\n \r\n// our texture\r\nuniform sampler2D u_image;\r\nuniform vec2 u_textureSize;\r\nuniform float u_kernel[9];\r\nuniform float u_kernelWeight;\r\n \r\n// the texCoords passed in from the vertex shader.\r\nvarying vec2 v_texCoord;\r\n \r\nvoid main() {\r\n   vec2 onePixel = vec2(1.0, 1.0) / u_textureSize;\r\n   vec4 colorSum =\r\n     texture2D(u_image, v_texCoord + onePixel * vec2(-1, -1)) * u_kernel[0] +\r\n     texture2D(u_image, v_texCoord + onePixel * vec2( 0, -1)) * u_kernel[1] +\r\n     texture2D(u_image, v_texCoord + onePixel * vec2( 1, -1)) * u_kernel[2] +\r\n     texture2D(u_image, v_texCoord + onePixel * vec2(-1,  0)) * u_kernel[3] +\r\n     texture2D(u_image, v_texCoord + onePixel * vec2( 0,  0)) * u_kernel[4] +\r\n     texture2D(u_image, v_texCoord + onePixel * vec2( 1,  0)) * u_kernel[5] +\r\n     texture2D(u_image, v_texCoord + onePixel * vec2(-1,  1)) * u_kernel[6] +\r\n     texture2D(u_image, v_texCoord + onePixel * vec2( 0,  1)) * u_kernel[7] +\r\n     texture2D(u_image, v_texCoord + onePixel * vec2( 1,  1)) * u_kernel[8] ;\r\n \r\n   // Divide the sum by the weight but just use rgb\r\n   // we'll set alpha to 1.0\r\n   gl_FragColor = vec4((colorSum / u_kernelWeight).rgb, 1.0);\r\n}\n"})}),"\n",(0,t.jsx)(r.p,{children:"In JavaScript we need to supply a convolution kernel and its weight"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-javascript",children:' function computeKernelWeight(kernel) {\r\n   var weight = kernel.reduce(function(prev, curr) {\r\n       return prev + curr;\r\n   });\r\n   return weight <= 0 ? 1 : weight;\r\n }\r\n \r\n ...\r\n var kernelLocation = gl.getUniformLocation(program, "u_kernel[0]");\r\n var kernelWeightLocation = gl.getUniformLocation(program, "u_kernelWeight");\r\n ...\r\n var edgeDetectKernel = [\r\n     -1, -1, -1,\r\n     -1,  8, -1,\r\n     -1, -1, -1\r\n ];\r\n gl.uniform1fv(kernelLocation, edgeDetectKernel);\r\n gl.uniform1f(kernelWeightLocation, computeKernelWeight(edgeDetectKernel));\r\n ...\n'})}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.img,{src:n(7502).A+"",width:"401",height:"301"})}),"\n",(0,t.jsx)(r.p,{children:"Here are a few more kernel effects"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-javascript",children:"  var kernels = {\r\n    normal: [\r\n        0, 0, 0,\r\n        0, 1, 0,\r\n        0, 0, 0\r\n    ],\r\n    gaussianBlur: [\r\n        0.045, 0.122, 0.045,\r\n        0.122, 0.332, 0.122,\r\n        0.045, 0.122, 0.045\r\n    ],\r\n    unsharpen: [\r\n        -1, -1, -1,\r\n        -1,  9, -1,\r\n        -1, -1, -1\r\n    ],\r\n    emboss: [\r\n        -2, -1,  0,\r\n        -1,  1,  1,\r\n        0,  1,  2\r\n    ]\r\n};\n"})}),"\n",(0,t.jsxs)(r.p,{children:["How about applying multiple effects? It could be done by generating shaders on the fly. Provide a UI that lets the user select the effects he wants to use then generate a shader that does all of the effects. That might not always be possible though that technique is often used to ",(0,t.jsx)(r.a,{href:"https://www.youtube.com/watch?v=cQUn0Zeh-0Q",children:"create effects for real time graphics"}),"."]}),"\n",(0,t.jsx)(r.p,{children:"A more flexible way is to use 2 more textures and render to each texture in turn, ping ponging back and forth and applying the next effect each time."}),"\n",(0,t.jsx)(r.p,{children:"Original Image  -> [Blur]            ->     Texture 1\r\nTexture 1       -> [Sharpen          ->     Texture 2\r\nTexture 2       -> [Edge Detect]     ->     Texture 1\r\nTexture 1       -> [Blur]            ->     Texture 2\r\nTexture 2       -> [Normal]          ->     Canvas"}),"\n",(0,t.jsx)(r.p,{children:"To do this we need to create framebuffers. In WebGL and OpenGL, a Framebuffer is actually a poor name. A WebGL/OpenGL Framebuffer is really just a collection of state (a list of attachments) and not actually a buffer of any kind. But, by attaching a texture to a framebuffer we can render into that texture."}),"\n",(0,t.jsx)(r.p,{children:"First let's turn the old texture creation code into a function"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-javascript",children:"function createAndSetupTexture(gl) {\r\n    var texture = gl.createTexture();\r\n    gl.bindTexture(gl.TEXTURE_2D, texture);\r\n    \r\n    // Set up texture so we can render any size image and so we are\r\n    // working with pixels.\r\n    gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_S, gl.CLAMP_TO_EDGE);\r\n    gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_WRAP_T, gl.CLAMP_TO_EDGE);\r\n    gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MIN_FILTER, gl.NEAREST);\r\n    gl.texParameteri(gl.TEXTURE_2D, gl.TEXTURE_MAG_FILTER, gl.NEAREST);\r\n    \r\n    return texture;\r\n}\r\n\r\n// Create a texture and put the image in it.\r\nvar originalImageTexture = createAndSetupTexture(gl);\r\ngl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, image);\n"})}),"\n",(0,t.jsx)(r.p,{children:"And now let's use that function to make two more textures and attach them to 2 framebuffers."}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-javascript",children:"// create 2 textures and attach them to framebuffers.\r\nvar textures = [];\r\nvar framebuffers = [];\r\nfor (var ii = 0; ii < 2; ++ii) {\r\n    var texture = createAndSetupTexture(gl);\r\n    textures.push(texture);\r\n    \r\n    // make the texture the same size as the image\r\n    gl.texImage2D(\r\n        gl.TEXTURE_2D, 0, gl.RGBA, image.width, image.height, 0,\r\n        gl.RGBA, gl.UNSIGNED_BYTE, null);\r\n    \r\n    // Create a framebuffer\r\n    var fbo = gl.createFramebuffer();\r\n    framebuffers.push(fbo);\r\n    gl.bindFramebuffer(gl.FRAMEBUFFER, fbo);\r\n    \r\n    // Attach a texture to it.\r\n    gl.framebufferTexture2D(gl.FRAMEBUFFER, gl.COLOR_ATTACHMENT0, gl.TEXTURE_2D, texture, 0);\r\n}\r\n\n"})}),"\n",(0,t.jsx)(r.p,{children:"Now let's make a set of kernels and then a list of them to apply."}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-javascript",children:'// Define several convolution kernels\r\nvar kernels = {\r\n    normal: [\r\n      0, 0, 0,\r\n      0, 1, 0,\r\n      0, 0, 0\r\n    ],\r\n    gaussianBlur: [\r\n      0.045, 0.122, 0.045,\r\n      0.122, 0.332, 0.122,\r\n      0.045, 0.122, 0.045\r\n    ],\r\n    unsharpen: [\r\n      -1, -1, -1,\r\n      -1,  9, -1,\r\n      -1, -1, -1\r\n    ],\r\n    emboss: [\r\n       -2, -1,  0,\r\n       -1,  1,  1,\r\n        0,  1,  2\r\n    ]\r\n};\r\n\r\n// List of effects to apply.\r\nvar effectsToApply = [\r\n    "gaussianBlur",\r\n    "emboss",\r\n    "unsharpen"\r\n];\n'})}),"\n",(0,t.jsx)(r.p,{children:"And finally let's apply each one, ping ponging which texture we are rendering too"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-javascript",children:'// start with the original image\r\ngl.bindTexture(gl.TEXTURE_2D, originalImageTexture);\r\n\r\n// loop through each effect we want to apply.\r\nfor (var ii = 0; ii < effectsToApply.length; ++ii) {\r\n    // Setup to draw into one of the framebuffers.\r\n    setFramebuffer(framebuffers[ii % 2], image.width, image.height);\r\n\r\n    drawWithKernel(effectsToApply[ii]);\r\n\r\n    // for the next draw, use the texture we just rendered to.\r\n    gl.bindTexture(gl.TEXTURE_2D, textures[ii % 2]);\r\n}\r\n\r\n// finally draw the result to the canvas.\r\nsetFramebuffer(null, canvas.width, canvas.height);\r\ndrawWithKernel("normal");\r\n\r\nfunction setFramebuffer(fbo, width, height) {\r\n    // make this the framebuffer we are rendering to.\r\n    gl.bindFramebuffer(gl.FRAMEBUFFER, fbo);\r\n\r\n    // Tell the shader the resolution of the framebuffer.\r\n    gl.uniform2f(resolutionUniformLocation, width, height);\r\n\r\n    // Tell webgl the viewport setting needed for framebuffer.\r\n    gl.viewport(0, 0, width, height);\r\n}\r\n\r\nfunction drawWithKernel(name) {\r\n    // set the kernel\r\n    gl.uniform1fv(kernelLocation, kernels[name]);\r\n    gl.uniform1f(kernelWeightLocation, computeKernelWeight(kernels[name]));\r\n\r\n    // Draw the rectangle.\r\n    gl.drawArrays(gl.TRIANGLES, 0, 6);\r\n}\n'})}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.img,{src:n(8691).A+"",width:"399",height:"299"})}),"\n",(0,t.jsxs)(r.p,{children:["Calling ",(0,t.jsx)(r.code,{children:"gl.bindFramebuffer"})," with ",(0,t.jsx)(r.code,{children:"null"})," tells WebGL you want to render to the canvas instead of to one of your framebuffers."]}),"\n",(0,t.jsxs)(r.p,{children:["WebGL has to convert from clip space back into pixels. It does this based on the settings of ",(0,t.jsx)(r.code,{children:"gl.viewport"}),". Since the framebuffers we are rendering into are a different size than the canvas we need to set the viewport appropriately when rendering to the framebuffer textures and then again when finally rendering to the canvas."]}),"\n",(0,t.jsxs)(r.p,{children:[(0,t.jsx)(r.strong,{children:"But why the image flipped?"})," It's because WebGL displays the canvas with 0,0 being the bottom left corner instead of the more traditional for 2D top left. That's not needed when rendering to a framebuffer. Because the framebuffer is never displayed, which part is top and bottom is irrelevant. All that matters is that pixel 0,0 in the framebuffer corresponds to 0,0 in our calculations. To deal with this I made it possible to set whether to flip or not by adding one more input into the shader."]}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-javascript",children:"...\r\nuniform float u_flipY;\r\n...\r\n\r\nvoid main() {\r\n...\r\n\r\n    gl_Position = vec4(clipSpace * vec2(1, u_flipY), 0, 1);\r\n\r\n...\r\n}\n"})}),"\n",(0,t.jsx)(r.p,{children:"And then we can set it when we render with"}),"\n",(0,t.jsx)(r.pre,{children:(0,t.jsx)(r.code,{className:"language-javascript",children:'    ...\r\n \r\n    var flipYLocation = gl.getUniformLocation(program, "u_flipY");\r\n \r\n    ...\r\n\r\n    // don\'t y flip images while drawing to the textures\r\n    gl.uniform1f(flipYLocation, 1);\r\n \r\n    ...\r\n \r\n    // need to y flip for canvas\r\n    gl.uniform1f(flipYLocation, -1);\n'})}),"\n",(0,t.jsx)(r.p,{children:(0,t.jsx)(r.img,{src:n(2366).A+"",width:"399",height:"300"})}),"\n",(0,t.jsx)(r.p,{children:"Try to add more convolution kernels and change effectsToApply to see what happens."})]})}function h(e={}){const{wrapper:r}={...(0,a.R)(),...e.components};return r?(0,t.jsx)(r,{...e,children:(0,t.jsx)(g,{...e})}):g(e)}},2234:(e,r,n)=>{n.d(r,{A:()=>t});const t=n.p+"assets/files/mr-survivor-7670d0ffeeb4a7488b1aef9416245cb9.jpg"},422:(e,r,n)=>{n.d(r,{A:()=>t});const t=n.p+"assets/images/image-processing-blur-1665781cc9c2dde36e2131ef82d6fe98.png"},2366:(e,r,n)=>{n.d(r,{A:()=>t});const t=n.p+"assets/images/image-processing-effects-2-799baae8d383bb2842c0ef824eb5d536.png"},8691:(e,r,n)=>{n.d(r,{A:()=>t});const t=n.p+"assets/images/image-processing-effects-1520ffba992f9c2c3469d7ef615d8ec4.png"},7502:(e,r,n)=>{n.d(r,{A:()=>t});const t=n.p+"assets/images/image-processing-kernel-52a1f25fee9ad155f8c24815d1903c6e.png"},1234:(e,r,n)=>{n.d(r,{A:()=>t});const t=n.p+"assets/images/image-processing-swap-color-105fa604213755c9351ef6ecd4e24bfb.png"},6256:(e,r,n)=>{n.d(r,{A:()=>t});const t=n.p+"assets/images/image-processing-texture-2450119d25565450a0442b0a98a505a4.png"},8453:(e,r,n)=>{n.d(r,{R:()=>o,x:()=>s});var t=n(6540);const a={},i=t.createContext(a);function o(e){const r=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function s(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:o(e.components),t.createElement(i.Provider,{value:r},e.children)}}}]);